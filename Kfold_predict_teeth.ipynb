{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets ,models,transforms\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Conv2d, MaxPool2d, Module\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "from dataloader_teeth_kfold import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from torch.nn import *\n",
    "from EarlyStopping import EarlyStopping\n",
    "from models import *\n",
    "#from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# batch_size = 16\n",
    "# train_dataset = GetDataset('','train')\n",
    "# val_dataset = GetDataset('','val')\n",
    "# test_dataset = GetDataset('','test',Istestori=True)\n",
    "# trainloader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "# valloader = DataLoader(val_dataset, batch_size=batch_size,shuffle=True)\n",
    "# testloader = DataLoader(test_dataset, batch_size=1,shuffle=False)\n",
    "# print('train :{}\\t,test :{}\\t,val :{}\\t'.format(len(train_dataset),len(test_dataset),len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 198, 148]             320\n",
      "            Conv2d-2         [-1, 32, 196, 146]           9,248\n",
      "       BatchNorm2d-3         [-1, 32, 196, 146]              64\n",
      "              ReLU-4         [-1, 32, 196, 146]               0\n",
      "         MaxPool2d-5           [-1, 32, 98, 73]               0\n",
      "            Conv2d-6           [-1, 64, 96, 71]          18,496\n",
      "       BatchNorm2d-7           [-1, 64, 96, 71]             128\n",
      "              ReLU-8           [-1, 64, 96, 71]               0\n",
      "         MaxPool2d-9           [-1, 64, 48, 35]               0\n",
      "           Conv2d-10          [-1, 128, 46, 33]          73,856\n",
      "      BatchNorm2d-11          [-1, 128, 46, 33]             256\n",
      "             ReLU-12          [-1, 128, 46, 33]               0\n",
      "        MaxPool2d-13          [-1, 128, 23, 16]               0\n",
      "           Conv2d-14          [-1, 128, 21, 14]         147,584\n",
      "      BatchNorm2d-15          [-1, 128, 21, 14]             256\n",
      "             ReLU-16          [-1, 128, 21, 14]               0\n",
      "        MaxPool2d-17           [-1, 128, 10, 7]               0\n",
      "           Linear-18                  [-1, 128]       1,147,008\n",
      "             ReLU-19                  [-1, 128]               0\n",
      "          Dropout-20                  [-1, 128]               0\n",
      "           Linear-21                   [-1, 64]           8,256\n",
      "             ReLU-22                   [-1, 64]               0\n",
      "          Dropout-23                   [-1, 64]               0\n",
      "           Linear-24                   [-1, 32]           2,080\n",
      "================================================================\n",
      "Total params: 1,407,552\n",
      "Trainable params: 1,407,552\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 46.41\n",
      "Params size (MB): 5.37\n",
      "Estimated Total Size (MB): 51.89\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=DeepConvNet().to(device)\n",
    "print(summary(model, (1, 200, 150)))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, fold_num):\n",
    "    \n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    classes = [i for i in range(1,33)]\n",
    "    confusion = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis] #normalize\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    im = ax.imshow(confusion, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set(xticks=np.arange(confusion.shape[1]),\n",
    "           yticks=np.arange(confusion.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    \n",
    "    fmt = '.2f'\n",
    "    thresh = confusion.max() / 2.\n",
    "    for i in range(confusion.shape[0]):\n",
    "        for j in range(confusion.shape[1]):\n",
    "            ax.text(j, i, format(confusion[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if confusion[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('plot/'+str(fold_num) + '_' + title + \".png\")\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(l, n):\n",
    "    return l[-n:] + l[:-n]\n",
    "\n",
    "def plot_acc_loss(train_acc,val_acc,train_loss,val_loss,fold_num):\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    plt.plot(train_acc,label='Train_acc')\n",
    "    plt.plot(val_acc,label='Val_acc')\n",
    "    plt.grid(True)\n",
    "    plt.title(str(fold_num))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy(%)')\n",
    "    plt.legend(loc='upper left',fontsize=8)\n",
    "    plt.savefig('plot/'+str(fold_num)+'cc.png')\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    plt.plot(train_loss,label='Train_loss')\n",
    "    plt.plot(val_loss,label='Val_loss')\n",
    "    plt.grid(True)\n",
    "    plt.title(str(fold_num))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper left',fontsize=8)\n",
    "    plt.savefig('plot/'+str(fold_num)+'loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nonflip(testloader, fold_num):\n",
    "    count = 0\n",
    "    df = pd.DataFrame(columns=['folder','image','label','predict','total_path'])\n",
    "    for idx, data in tqdm(enumerate(testloader)):\n",
    "        img, label, img_path = data[0].to(device, dtype=torch.float32), data[1].to(device, dtype=torch.int64), data[2]\n",
    "        if img_path[0][-5] == 'F':\n",
    "            continue\n",
    "        x, y = data[0].to(device, dtype=torch.float32), data[1].to(device, dtype=torch.int64)\n",
    "        output = model(x)\n",
    "        max_value, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        if len(img_path[0].split('_'))==14:\n",
    "            folder = img_path[0].split('_')[-7]\n",
    "        if len(img_path[0].split('_'))==15:\n",
    "            folder = img_path[0].split('_')[-8] + '_' + img_path[0].split('_')[-7]\n",
    "\n",
    "        #image = img_path[0].split('_')[-3]\n",
    "        image = img_path[0].split('_')[-6]+'_'+img_path[0].split('_')[-5]+'_'+img_path[0].split('_')[-4]+'_'+img_path[0].split('_')[-3]\n",
    "\n",
    "        \n",
    "        if folder[-3:] == '_NN':\n",
    "            folder = folder[:-3]\n",
    "            image = 'NN_' + image        \n",
    "    \n",
    "        label = label.cpu().numpy()[0]+1\n",
    "        predict = predicted.cpu().numpy()[0]+1\n",
    "        df.loc[count] = [folder, image, label, predict, img_path[0]]\n",
    "        count += 1\n",
    "\n",
    "    df = df.set_index(keys = ['folder','image'])\n",
    "    df.sort_values(['folder', 'image','label'],inplace=True)\n",
    "\n",
    "    for idx in df.index.unique():\n",
    "        if df.loc[idx]['label'][0] > 16:\n",
    "            df.loc[idx] = df.loc[idx].sort_values(['folder', 'image','label'],ascending=[True, True, False])\n",
    "    df.to_csv('kfold/'+str(fold_num)+'_non_flip'+'.csv')\n",
    "    \n",
    "    len_dataframe = pd.DataFrame(columns=['folder','image','length'])\n",
    "    for idx, folder_image in enumerate(df.index.unique()):\n",
    "        len_dataframe.loc[idx] = [folder_image[0],folder_image[1],len(df.loc[folder_image])]\n",
    "    len_dataframe = len_dataframe.set_index(keys = ['folder','image'])\n",
    "    len_dataframe.to_csv('kfold/'+str(fold_num)+'len_dataframe.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revised_by_table(radiograph, uni):\n",
    "    if (uni==4) or (uni==5):\n",
    "        radiograph.iloc[0] = [4,13,True]\n",
    "        radiograph.iloc[1] = [5,12,True]\n",
    "    elif (uni==8) or (uni==9):\n",
    "        radiograph.iloc[0] = [8,9,True]\n",
    "        radiograph.iloc[1] = [9,8,True]\n",
    "    elif (uni==12) or (uni==13):\n",
    "        radiograph.iloc[0] = [12,5,True]\n",
    "        radiograph.iloc[1] = [13,4,True]\n",
    "    elif (uni==20) or (uni==21):\n",
    "        radiograph.iloc[0] = [21,28,True]\n",
    "        radiograph.iloc[1] = [20,29,True]\n",
    "    elif (uni==28) or (uni==29):\n",
    "        radiograph.iloc[0] = [29,20,True]\n",
    "        radiograph.iloc[1] = [28,21,True]\n",
    "    return radiograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_point_table(radiograph):\n",
    "    point_table = np.zeros(len(radiograph),dtype=int)\n",
    "    #print('len(point_table) =',len(radiograph))\n",
    "    \n",
    "    upper = 0\n",
    "    lower = 0\n",
    "    for radio_idx, radio_row in radiograph.iterrows():#上下排預測\n",
    "        if radio_row['p'] < 17:\n",
    "            upper +=1\n",
    "        else:\n",
    "            lower += 1\n",
    "        if radio_row['fp'] < 17:\n",
    "            upper +=1\n",
    "        else:\n",
    "            lower += 1\n",
    "#     if radiograph['p'].mean() < 17:\n",
    "    if upper >= lower:\n",
    "        v_value = 17\n",
    "        for radio_idx, radio_row in radiograph.iterrows():#加起來17\n",
    "            if radio_row['p']+radio_row['fp'] == 17:\n",
    "                point_table[radio_idx] += 1\n",
    "\n",
    "        for i in range(len(radiograph)-1): #正向分數\n",
    "            for j in range(i+1,len(radiograph)):\n",
    "                if radiograph.iloc[j]['p'] - radiograph.iloc[i]['p'] == j - i:\n",
    "                    point_table[i] += 1\n",
    "                    point_table[j] += 1\n",
    "\n",
    "        for i in range(len(radiograph)-1): #逆向分數\n",
    "            for j in range(i+1,len(radiograph)):\n",
    "                if radiograph.iloc[j]['fp'] - radiograph.iloc[i]['fp'] == i - j:\n",
    "                    point_table[i] += 1\n",
    "                    point_table[j] += 1\n",
    "    else:\n",
    "        v_value = 49\n",
    "        for radio_idx, radio_row in radiograph.iterrows():#加起來49\n",
    "            if radio_row['p']+radio_row['fp'] == 49:\n",
    "                point_table[radio_idx] += 1\n",
    "\n",
    "        for i in range(len(radiograph)-1):\n",
    "            for j in range(i+1,len(radiograph)):\n",
    "                if radiograph.iloc[j]['p'] - radiograph.iloc[i]['p'] == i - j:\n",
    "                    point_table[i] += 1\n",
    "                    point_table[j] += 1\n",
    "\n",
    "        for i in range(len(radiograph)-1):\n",
    "            for j in range(i+1,len(radiograph)):\n",
    "                if radiograph.iloc[j]['fp'] - radiograph.iloc[i]['fp'] == j - i:\n",
    "                    point_table[i] += 1\n",
    "                    point_table[j] += 1\n",
    "\n",
    "    #print(point_table)\n",
    "    return (point_table, v_value)\n",
    "\n",
    "def revised(radiograph, point_table, v_value):\n",
    "    max_num = point_table.max()\n",
    "    if len(point_table) == 1:\n",
    "        return radiograph\n",
    "    \n",
    "    base = 0\n",
    "    max_prob = 0\n",
    "    #print(radiograph)\n",
    "    for i in range(len(point_table)):\n",
    "        if point_table[i]==max_num:\n",
    "            prob_cur = radiograph.iloc[i]['prob_p'] + radiograph.iloc[i]['prob_fp']\n",
    "            if max_prob < prob_cur:\n",
    "                max_prob = prob_cur\n",
    "                base = i\n",
    "            #break\n",
    "    \n",
    "    if v_value == 17:\n",
    "        for radio_idx in range(len(point_table)):\n",
    "            gap = radio_idx - base\n",
    "            revise_value = radiograph.iloc[base]['p']+gap\n",
    "            radiograph.iloc[radio_idx] = [revise_value,v_value-revise_value,radiograph.iloc[radio_idx]['check'],radiograph.iloc[radio_idx]['prob_p'],radiograph.iloc[radio_idx]['prob_fp']]\n",
    "#             if point_table[radio_idx]!=max_num:\n",
    "#                 gap = radio_idx - base\n",
    "#                 revise_value = radiograph.iloc[base]['p']+gap\n",
    "#                 radiograph.iloc[radio_idx] = [revise_value,v_value-revise_value,radiograph.iloc[radio_idx]['check'],radiograph.iloc[radio_idx]['prob_p'],radiograph.iloc[radio_idx]['prob_fp']]\n",
    "#             else:\n",
    "#                 base = radio_idx\n",
    "    else:\n",
    "        for radio_idx in range(len(point_table)):\n",
    "                gap = radio_idx - base\n",
    "                revise_value = radiograph.iloc[base]['p']-gap\n",
    "                radiograph.iloc[radio_idx] = [revise_value,v_value-revise_value,radiograph.iloc[radio_idx]['check'],radiograph.iloc[radio_idx]['prob_p'],radiograph.iloc[radio_idx]['prob_fp']]\n",
    "#             if point_table[radio_idx]!=max_num:\n",
    "#                 gap = radio_idx - base\n",
    "#                 revise_value = radiograph.iloc[base]['p']-gap\n",
    "#                 radiograph.iloc[radio_idx] = [revise_value,v_value-revise_value,radiograph.iloc[radio_idx]['check'],radiograph.iloc[radio_idx]['prob_p'],radiograph.iloc[radio_idx]['prob_fp']] \n",
    "#             else:\n",
    "#                 base = radio_idx \n",
    "            \n",
    "    return radiograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm(fold_num):\n",
    "    new_size = (200,150)\n",
    "    trans_method = [\n",
    "        transforms.Resize(new_size),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    "    trans_flipback = [\n",
    "        transforms.Resize(new_size),\n",
    "        transforms.RandomVerticalFlip(p=1),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    "    trans = transforms.Compose(trans_method)\n",
    "    trans_back = transforms.Compose(trans_flipback)\n",
    "    \n",
    "    len_dataframe = pd.read_csv('kfold/'+str(fold_num)+'len_dataframe.csv')\n",
    "    len_dataframe = len_dataframe.set_index(keys = ['folder','image'])\n",
    "    df = pd.read_csv('kfold/'+str(fold_num)+'_non_flip'+'.csv')\n",
    "    df = df.set_index(keys = ['folder','image'])\n",
    "    \n",
    "    model = torch.load('./model/'+'fold'+str(fold_num)+'/best.pkl')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#=================================================================    \n",
    "    count = 0\n",
    "    cur_index = len_dataframe.iloc[count].values[0]\n",
    "\n",
    "    out_df = pd.DataFrame(columns=['folder','image','label','flip_label','p','fp','check'])\n",
    "    out_df_idx = 0\n",
    "\n",
    "    radiograph = pd.DataFrame(columns=['p','fp','check','prob_p','prob_fp'])\n",
    "    radiograph_idx = 0\n",
    "    ori_df = pd.DataFrame(columns=['folder','image','ori_p','ori_fp','prob_p','prob_fp'])\n",
    "\n",
    "    total_point_table = []\n",
    "\n",
    "    for idx, row in enumerate(df.iterrows()):\n",
    "        if idx >= cur_index: # next radiograph\n",
    "\n",
    "            radiograph_idx = 0\n",
    "            radiograph = pd.DataFrame(columns=['p','fp','check','prob_p','prob_fp'])\n",
    "\n",
    "            count += 1\n",
    "            cur_index += len_dataframe.iloc[count].values[0]\n",
    "        img_path = row[1]['total_path']\n",
    "        img = PIL.Image.open(img_path).convert('RGB')\n",
    "        flip_img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        label = row[1]['label']\n",
    "        if label < 17:\n",
    "            flabel = 17 - label\n",
    "            img = trans_back(img)\n",
    "            flip_img = trans_back(flip_img)\n",
    "        else:\n",
    "            flabel = 49 - label\n",
    "            img = trans(img)\n",
    "            flip_img = trans(flip_img)\n",
    "\n",
    "        img = img.to(device)\n",
    "        img.unsqueeze_(0)\n",
    "        flip_img = flip_img.to(device)\n",
    "        flip_img.unsqueeze_(0)\n",
    "\n",
    "        output = model(img)\n",
    "\n",
    "        max_value, predicted = torch.max(output.data, 1)\n",
    "        prob_p = F.softmax(output.data,dim=1).cpu().numpy().max()\n",
    "        \n",
    "        output = model(flip_img)\n",
    "        max_value, flip_predicted = torch.max(output.data, 1)\n",
    "        prob_fp = F.softmax(output.data,dim=1).cpu().numpy().max()\n",
    "        \n",
    "        p = predicted.cpu().numpy()[0]+1\n",
    "        fp = flip_predicted.cpu().numpy()[0]+1\n",
    "\n",
    "        ori_df.loc[idx] = [row[0][0],row[0][1],p,fp,prob_p,prob_fp]\n",
    "        \n",
    "        if (p+fp==17) | (p+fp==49):\n",
    "            check = True\n",
    "        else:\n",
    "            check = False\n",
    "\n",
    "        radiograph.loc[radiograph_idx] = [p, fp, check, prob_p, prob_fp]\n",
    "        radiograph_idx += 1\n",
    "        if idx == cur_index-1: # do rule operation\n",
    "            #print('size =',len_dataframe.iloc[count].values[0])\n",
    "\n",
    "#             print(row[0])\n",
    "\n",
    "            #全錯先不處理\n",
    "            if len(radiograph[radiograph['check']==False]) == len(radiograph):\n",
    "                for i in range(len(radiograph)):\n",
    "                    \n",
    "                    label = df.loc[row[0],['label']].iloc[i][0]\n",
    "                    if label < 17:\n",
    "                        flabel = 17 - label\n",
    "                    else:\n",
    "                        flabel = 49 - label\n",
    "                        \n",
    "                    p = radiograph.iloc[i]['p']\n",
    "                    fp = radiograph.iloc[i]['fp']\n",
    "                    if p < 17:\n",
    "                        v_value = 17\n",
    "                    else:\n",
    "                        v_value = 49\n",
    "                    if radiograph.iloc[i]['prob_p']>radiograph.iloc[i]['prob_fp']:\n",
    "                        radiograph.iloc[i] = [p,v_value-p,radiograph.iloc[i]['check'],radiograph.iloc[i]['prob_p'],radiograph.iloc[i]['prob_fp']]\n",
    "                    else:\n",
    "                        radiograph.iloc[i] = [v_value-fp,fp,radiograph.iloc[i]['check'],radiograph.iloc[i]['prob_p'],radiograph.iloc[i]['prob_fp']]\n",
    "                    \n",
    "#                     out_df.loc[out_df_idx] = [row[0][0],row[0][1],label,flabel,radiograph.iloc[i]['p'],radiograph.iloc[i]['fp'],radiograph.iloc[i]['check']]\n",
    "#                     out_df_idx += 1\n",
    "#                     total_point_table+=[999]\n",
    "                    \n",
    "                        \n",
    "#                     label = df.loc[row[0],['label']].iloc[i][0]\n",
    "#                     if label < 17:\n",
    "#                         flabel = 17 - label\n",
    "#                     else:\n",
    "#                         flabel = 49 - label\n",
    "\n",
    "#                     out_df.loc[out_df_idx] = [row[0][0],row[0][1],label,flabel,radiograph.iloc[i]['p'],radiograph.iloc[i]['fp'],radiograph.iloc[i]['check']]\n",
    "#                     out_df_idx += 1\n",
    "\n",
    "#                     total_point_table+=[999]\n",
    "\n",
    "#                 continue\n",
    "\n",
    "            point_table, v_value = count_point_table(radiograph)\n",
    "\n",
    "            radiograph = revised(radiograph, point_table, v_value)\n",
    "\n",
    "            if (len(radiograph[radiograph['check']==True]) == len(radiograph)) and (len(radiograph)==2):\n",
    "                if len(radiograph['p'].unique()) == 1:\n",
    "                    uni = radiograph['p'].unique()[0]\n",
    "                    radiograph = revised_by_table(radiograph,uni)\n",
    "\n",
    "            for i in range(len(radiograph)):\n",
    "                label = df.loc[row[0],['label']].iloc[i][0]\n",
    "                if label < 17:\n",
    "                    flabel = 17 - label\n",
    "                else:\n",
    "                    flabel = 49 - label\n",
    "                out_df.loc[out_df_idx] = [row[0][0],row[0][1],label,flabel,radiograph.iloc[i]['p'],radiograph.iloc[i]['fp'],radiograph.iloc[i]['check']]\n",
    "                out_df_idx += 1\n",
    "            total_point_table += point_table.tolist()\n",
    "            radiograph['ori_p'] = df.loc[row[0],['label']].values\n",
    "            radiograph['ori_fp'] = v_value - df.loc[row[0],['label']].values\n",
    "            \n",
    "#             if row[0][1] == '15C4DA':\n",
    "#                 print(pd.DataFrame(radiograph))\n",
    "#             print()\n",
    "#     print(radiograph)\n",
    "    out_df['point'] = total_point_table\n",
    "    out = pd.concat([out_df,ori_df[['ori_p','ori_fp','prob_p','prob_fp']]],axis=1)\n",
    "    for radio_idx, radio_row in out_df.iterrows():\n",
    "        if radio_row['check'] == True:\n",
    "            out_df.loc[radio_idx,'check'] = 1\n",
    "    out_df = out_df.reindex(columns=['folder','image','label','p','flip_label','fp','check','point'])\n",
    "    out_df = out_df.set_index(keys = ['folder','image'])\n",
    "    out_df.to_csv('kfold/'+str(fold_num)+'_revised_prediction.csv')\n",
    "    ori_df = ori_df.set_index(keys = ['folder','image'])\n",
    "    out = out.reindex(columns=['folder','image','label','p','flip_label','fp','check','ori_p','ori_fp','point','prob_p','prob_fp'])\n",
    "    out = out.set_index(keys = ['folder','image'])\n",
    "    out.to_csv('kfold/'+str(fold_num)+'_ori_pred.csv')\n",
    "    acc_p = 0\n",
    "    acc_fp = 0\n",
    "    for idx, row in out.iterrows():\n",
    "        if row['ori_p'] == row['label']:\n",
    "            acc_p += 1\n",
    "        if row['ori_fp'] == row['flip_label']:\n",
    "            acc_fp += 1\n",
    "    acc = acc_p + acc_fp\n",
    "    print('=============original acc===============')\n",
    "    print(acc_p/len(out_df), acc_fp/len(out_df),acc / (2*len(out_df)))\n",
    "    acc_p = 0\n",
    "    acc_fp = 0\n",
    "    for idx, row in out.iterrows():\n",
    "        if row['p'] == row['label']:\n",
    "            acc_p += 1\n",
    "        if row['fp'] == row['flip_label']:\n",
    "            acc_fp += 1\n",
    "    acc = acc_p + acc_fp\n",
    "    print(acc_p/len(out_df), acc_fp/len(out_df),acc / (2*len(out_df)))\n",
    "    \n",
    "    x = np.concatenate((out_df['p'].values.astype(int),out_df['fp'].values.astype(int)))\n",
    "    y = np.concatenate((out_df['label'].values.astype(int),out_df['flip_label'].values.astype(int)))\n",
    "    print(max(x),min(x))\n",
    "    plot_confusion_matrix(y, x,'revised_confusion',fold_num)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(testloader, fold_num):\n",
    "    model = torch.load('./model/'+'fold'+str(fold_num)+'/best.pkl')\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    targets = []\n",
    "    preds = []\n",
    "    test_path = []\n",
    "    for i, data in tqdm(enumerate(testloader)):\n",
    "        x, y, img_path = data[0].to(device, dtype=torch.float32), data[1].to(device, dtype=torch.int64), data[2]\n",
    "        #test_path = test_path.append(data[2])\n",
    "\n",
    "        output = model(x)\n",
    "        max_value, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += y.size(0) #batch_size\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "        targets.extend(y.view_as(predicted))\n",
    "        preds.extend(predicted)\n",
    "\n",
    "    print('Maxacc : %.4f'%(100*correct/total))\n",
    "    targets_val = torch.stack(targets)\n",
    "    preds_val = torch.stack(preds)\n",
    "    plot_confusion_matrix(targets_val.cpu().numpy(), preds_val.cpu().numpy(),'original_confusion',fold_num)\n",
    "    create_nonflip(testloader, fold_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "def train(trainloader,valloader,epochs,learning_rate, patience,fold_num):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    # model\n",
    "    model=DeepConvNet().to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-3)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    train_acc=[]\n",
    "    train_loss=[]\n",
    "    \n",
    "    val_acc=[]\n",
    "    val_loss=[]\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    eval_total = 0\n",
    "    eval_correct = 0\n",
    "    \n",
    "    max_vacc = 0\n",
    "    \n",
    "    print(\"Starting the training loop from epoch {}\".format(0))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print('train')\n",
    "        running_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data in tqdm(trainloader):\n",
    "            \n",
    "            # input\n",
    "            x, y = data[0].to(device, dtype=torch.float32), data[1].to(device, dtype=torch.int64)\n",
    "#             print(x.shape)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            output = model(x) # forward      \n",
    "            \n",
    "            #Calculate accuracy\n",
    "            max_value, predicted = torch.max(output.data, 1)\n",
    "            total += y.size(0) #batch_size\n",
    "            correct += (predicted == y).sum().item()\n",
    "            \n",
    "            loss = criterion(output, y) #loss\n",
    "            \n",
    "            loss.backward() # backward\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        tloss=running_loss/len(trainloader)\n",
    "        train_loss.append(tloss)\n",
    "        \n",
    "        tacc = correct/total\n",
    "        train_acc.append(tacc)\n",
    "        \n",
    "        #==========eval===========\n",
    "        model.eval()\n",
    "#        print('eval')\n",
    "        eval_total = 0\n",
    "        eval_correct = 0\n",
    "        val_running_loss = 0\n",
    "        for data in tqdm(valloader):\n",
    "            # input\n",
    "            x, y = data[0].to(device, dtype=torch.float32), data[1].to(device, dtype=torch.int64)\n",
    "#             optimizer.zero_grad()\n",
    "            output = model(x) # forward\n",
    "            #Calculate accuracy\n",
    "            max_value, predicted = torch.max(output.data, 1)\n",
    "            eval_total += y.size(0) #batch_size\n",
    "            eval_correct += (predicted == y).sum().item()\n",
    "            \n",
    "            loss = criterion(output, y) #loss\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "        \n",
    "        vloss=val_running_loss/len(valloader)\n",
    "        val_loss.append(vloss)\n",
    "        \n",
    "        eval_acc = eval_correct/eval_total\n",
    "        val_acc.append(eval_acc)\n",
    "        \n",
    "        if max_vacc < eval_acc:\n",
    "            max_vacc = eval_acc\n",
    "            torch.save(model, 'model/'+'fold'+str(fold_num)+'/'+'best'+'.pkl')\n",
    "        \n",
    "        print('Epoch[%d] Loss: %.4f'% (epoch+1, tloss))\n",
    "        print('Epoch[%d] Acc: %.4f'% (epoch+1, tacc))\n",
    "        print('Epoch[%d] eval_Acc: %.4f'% (epoch+1, eval_acc))        \n",
    "        \n",
    "        if int(eval_acc*100) >= 75:\n",
    "            torch.save(model, 'model/'+'fold'+str(fold_num)+'/'+str(int(eval_acc*100))+'.pkl')\n",
    "        \n",
    "        \n",
    "    print('max_eval_acc =,',max_vacc)\n",
    "    return train_acc,train_loss,val_acc,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====0===== \n",
      "data length = 1912\n",
      "> Found 1912 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lab118\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3adcead4cb94a70aed3c25be1ebd247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maxacc : 80.9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lab118\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ddb6083e1e42d6920e8493819d4807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============original acc===============\n",
      "0.805439330543933 0.8127615062761506 0.8091004184100419\n",
      "0.8880753138075314 0.8880753138075314 0.8880753138075314\n",
      "32 1\n",
      "=====1===== \n",
      "data length = 1910\n",
      "> Found 1910 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df62a7027ccd4c509c3ddb2ee3c1cfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maxacc : 80.1571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29feca2dbf99497cbaf2c0650392f779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============original acc===============\n",
      "0.7979057591623037 0.8052356020942408 0.8015706806282723\n",
      "0.8795811518324608 0.8795811518324608 0.8795811518324608\n",
      "32 2\n",
      "=====2===== \n",
      "data length = 1910\n",
      "> Found 1910 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b258e671634b259754ad5d2f81db23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maxacc : 80.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c86f8ba2b13481da642891eaad52806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============original acc===============\n",
      "0.8020942408376963 0.7979057591623037 0.8\n",
      "0.8764397905759163 0.8764397905759163 0.8764397905759163\n",
      "32 1\n",
      "=====3===== \n",
      "data length = 1910\n",
      "> Found 1910 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71b257cf8734766ab57b85141919cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maxacc : 78.9005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bedbce6b80495f9dc123435dbb8d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============original acc===============\n",
      "0.7895287958115184 0.7884816753926701 0.7890052356020942\n",
      "0.8858638743455497 0.8848167539267016 0.8853403141361257\n",
      "32 -12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lab118\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====4===== \n",
      "data length = 1910\n",
      "> Found 1910 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e3aa7115144a69b17c7614aa6dfaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maxacc : 80.4712\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddcc4e3271643ccb12b1eaa1704c0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============original acc===============\n",
      "0.806282722513089 0.8031413612565445 0.8047120418848167\n",
      "0.8837696335078534 0.8837696335078534 0.8837696335078534\n",
      "32 1\n",
      "Wall time: 4min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fold_nums = 5\n",
    "ori_folds = [0,1,2,3,4]\n",
    "choice_fold = [0,1,2,3,4]\n",
    "# folds = rotate(folds,2)\n",
    "batch_size = 16\n",
    "n_epochs = 50\n",
    "patience = 20\n",
    "for fold_num in choice_fold:\n",
    "    \n",
    "    print('====='+str(fold_num)+ '===== ')\n",
    "    folds = rotate(ori_folds,fold_num)\n",
    "#     train_dataset = GetDataset('','train',folds=folds)\n",
    "#     val_dataset = GetDataset('','val',folds=folds)\n",
    "    test_dataset = GetDataset('','test',Istestori=True,folds=folds)\n",
    "#     trainloader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "#     valloader = DataLoader(val_dataset, batch_size=batch_size,shuffle=True)\n",
    "    testloader = DataLoader(test_dataset, batch_size=1,shuffle=False)\n",
    "    \n",
    "#     train_acc,train_loss,val_acc,val_loss = train(trainloader,valloader,n_epochs,0.001, patience, fold_num)\n",
    "#     plot_acc_loss(train_acc,val_acc,train_loss,val_loss,fold_num)\n",
    "    evaluate(testloader, fold_num)\n",
    "    algorithm(fold_num)\n",
    "    \n",
    "#     folds = rotate(folds,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-7342ded82478>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-7342ded82478>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    0.8880753138075314 0.8785340314136125 0.875392670157068 0.8853403141361257 0.8806282722513089\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "0.8880753138075314 0.8785340314136125 0.875392670157068 0.8853403141361257 0.8806282722513089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8859832635983264 0.8774869109947644 0.8701570680628272 0.8842931937172774 0.8774869109947644\n",
    "0.8817991631799164 0.8795811518324608 0.8691099476439791 0.8832460732984293 0.8785340314136125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
